---
title: "Intro to ML - Project (Churn)"
author: "Abhinav, Archit, Ayush, Nir, Matthew"
date: "7/18/2021"
output: html_document
number_sections : true

---
Setting up
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
#########################
##Cleaning Data and EDA##
#########################
#Install libraries if not installed, else load them-----------------------------
ipak <- function(pkg){
  new.pkg <- pkg[!(pkg %in% installed.packages()[, "Package"])]
  if (length(new.pkg)) 
    install.packages(new.pkg, dependencies = TRUE)
  sapply(pkg, require, character.only = TRUE)
}
# usage
packages <- c("ggplot2", "ISLR", "DataExplorer", "RColorBrewer", "dplyr", "data.table","rpart","randomForest","xgboost","DescTools","Hmisc","ggcorrplot","MASS","tidyverse","caret","precrec","Matrix","gbm","ROCR")
ipak(packages)

options(scipen=999)

#Set seed and working directory-------------------------------------------------
set.seed(100)
#setwd("C:/Users/archi/OneDrive/Documents/GitHub/STA380-69963/ML - Project 1")#--WD for Archit
#setwd("~/Documents/GitHub/Bank-Customer-Churn-Prediction")#--WD for Abhinav
setwd("C:\\Users\\nirra\\OneDrive\\Documents\\GitHub\\Bank-Customer-Churn-Prediction")#WD-- for nir
```


Exploring data
```{r}
#Read data file
raw_data = fread('Churn_Modelling.csv')
raw_data=Churn_Modelling

n = dim(raw_data)[1]
df = data.frame(raw_data)

#Understanding the structure of data
str(df)
```

```{r}
#Checking if there are any null value in the dataframe
sapply(df,function(df) sum(is.na(df)))
```

```{r}
#Checking Unique value counts in each columns
sapply(df, n_distinct)

#Exploring unique values for few variables
unique(df$NumOfProducts)
unique(df$HasCrCard)
unique(df$IsActiveMember)
unique(df$Exited)
```


```{r}

## To make data frames easily readable we have 
## removed unnecessary data fields
df = subset(raw_data, select = -c(RowNumber,CustomerId,Surname))
df = data.frame(df)

#Coerce response variable to factor
df$Exited = factor(as.character(df$Exited),levels = c("0","1"))

#Plotting Histograms to understand the distributions
par(mfrow = c(3, 2))

hist(df$CreditScore,main=c("Credit Score"),xlab = c(""),col = "lightblue")
hist(df$Age,main=c("Age"),xlab = c(""),col = "lightblue")
hist(df$Tenure, breaks = 10,main=c("Tenure"),xlab = c(""),col = "lightblue")
hist(df$Balance, breaks = 12,main=c("Balance"),xlab = c(""),col = "lightblue")
hist(df$EstimatedSalary,main=c("Estimated Salary"),xlab = c(""),col = "lightblue")
```
We observe more or less gaussian distribution for credit score and age (with slight right skew) whereas Tenure and Estimated salary more or less are uniform in distribution. A big peak is seen in balance variable distribution at zero implying quite a lot of customers with zero balance accounts.

```{r}
#Plotting Bar Charts to understand the Categorical Variables
theme_custom <- function () { 
    theme_bw(base_size=12, base_family="Avenir") %+replace% 
        theme(
            panel.background  = element_blank(),
            plot.background = element_rect(fill="gray96", colour=NA), 
            legend.background = element_rect(fill="transparent", colour=NA),
            legend.key = element_rect(fill="transparent", colour=NA)
        )
}

ggplot(df, aes(x = factor(Gender))) + geom_bar(fill="skyblue2",alpha=0.65) +xlab("Gender")+
  geom_text(aes(label = ..count..), stat = "count", vjust = 1.5, colour = "black")+theme_custom()

ggplot(df, aes(x = factor(Geography))) + geom_bar(fill="skyblue2",alpha=0.65) +xlab("Geography")+
  geom_text(aes(label = ..count..), stat = "count", vjust = 1.5, colour = "black")+theme_custom()

ggplot(df, aes(x = factor(NumOfProducts))) + geom_bar(fill="skyblue2",alpha=0.65)+xlab("Number of Products")+
  geom_text(aes(label = ..count..), stat = "count", vjust = 1.5, colour = "black")+theme_custom()

ggplot(df, aes(x = factor(HasCrCard))) + geom_bar(fill="skyblue2",alpha=0.65) +xlab("Has Credit Card")+
  geom_text(aes(label = ..count..), stat = "count", vjust = 1.5, colour = "black")+theme_custom()

ggplot(df, aes(x = factor(IsActiveMember)))+ geom_bar(fill="skyblue2",alpha=0.65)+xlab("Active membership")+
  geom_text(aes(label = ..count..), stat = "count", vjust = 1.5, colour = "black")+theme_custom()

ggplot(df, aes(x = Exited)) + geom_bar(fill="skyblue2",alpha=0.65) +
  geom_text(aes(label = ..count..), stat = "count", vjust = 1.5, colour = "black")+
  labs( x = "Exit Status")+theme_custom()

```
We observe majority customers are using credit card and roughly half of them have active membership. Rate of customer exit is 20%.

```{r}
#Customer Churn by Region
#table1 <- table(df$Exited, df$Geography, dnn=c("Exit Count", "Geography")) 
#barplot(table1, ylab="Frequency", xlab="Geography", main="Comparing Exit Status across countries\n", 
#        col=c("turquoise4", "turquoise2" ), beside=TRUE, width=.2)
#legend("right", title="Exited", legend= sort(unique(df$Exited)),
#       fill =c("turquoise4", "turquoise2" ), box.lty=0)

```


```{r}
## Geography vs Gender
cols <- c("Gender","Geography","NumOfProducts","HasCrCard","IsActiveMember","Exited" )
df[cols] <- lapply(df[cols], function(x) as.factor(as.character(x)))

g <- ggplot(df, aes(x = Geography)) +geom_bar(aes(fill = Gender),position="dodge")+geom_text(aes(label = ..count..),position = "identity", stat = "count", vjust =5 , colour = "black")+scale_fill_manual(values=c('#999999','#E69F00'))+theme_custom()
plotly::ggplotly(g+ggtitle("Gender Distribution by Geography"))



```
We see similar gender distributions across all the countries.

```{r}
## Customer Churn by Geography
plotly::ggplotly(ggplot(df, aes(x = Geography, fill = Exited)) +
  geom_bar(position="dodge") +
  geom_text(aes(label = ..count..),
            stat = "Count",position = position_dodge(0.8),
            vjust = 1.5, hjust = 0.5,
            colour = "black") +
  scale_fill_manual(values=c('#999999','#E69F00'))+
  labs(title = "Churn by Geography")+theme_custom())
```

France despite having almost half of the customers from our sample has fewer exits. Germany on the contrary sees more attrition.

```{r}
##Trying to plot in percent---WIP
## Customer Churn by Gender
plotly::ggplotly(ggplot(df, aes(x = Gender,y = (..count..)/sum(..count..), fill = Exited)) +
  geom_bar(position="dodge") +
  geom_text(aes(label =scales::percent( (..count..)/sum(..count..)) ,y = (..count..)/sum(..count..)),
            stat = "Count",position = position_dodge(0.8),
            vjust = 1.5, hjust = 0.9,
            colour = "black") +
  scale_fill_manual(values=c('#999999','#E69F00'))+
  labs(title = "Churn by Gender")+theme_custom())

```
Roughly one in every three women versus one in every five men are exiting the concerned banking services.

```{r}
#Density Plots
plotly::ggplotly(ggplot(df, aes(x=Age)) + 
  geom_density(fill="grey",alpha=0.65)+theme_custom())

plotly::ggplotly(ggplot(df, aes(x=Age,fill=Exited),size=1.3) + 
  geom_density(alpha=0.65)+
  labs(title = "Density Plot: Age")+theme_custom())

#ggplot(df, aes(x=NumOfProducts)) +geom_density()+scale_color_manual(values=c("#999999", "#E69F00", "#56B4E9"))+labs(title = "Density Plot: #Products")

#ggplot(df, aes(x=NumOfProducts, color=Exited)) + geom_density()+scale_color_manual(values=c("#999999", "#E69F00", "#56B4E9"))+labs(title = "Density Plot: #Products")

```
```{r include=FALSE}
df$Exited <- as.factor(df$Exited)
ggplot(df, aes(x=NumOfProducts, color=Exited)) +
  geom_density()+
  scale_color_manual(values=c("#999999", "#E69F00", "#56B4E9"))+
  labs(title = "Density Plot: #Products")+xlab("Number of Products") + theme_custom()
```

Age may as well be a good predictor given the observable difference in mean Age for customers retained vs lost.

```{r}
#Correlation Analysis

#str(df)
cols_num <- c("CreditScore","Age","Tenure","NumOfProducts","Balance","EstimatedSalary" )
df[cols_num] <- lapply(df[cols_num], function(x) as.numeric(x))
corr <- round(cor(df[cols_num]), 2)
#ggcorrplot(
#  corr,
#  hc.order = TRUE,
#  type = "lower",
#  outline.color = "white",
#  ggtheme = ggplot2::theme_gray,
#  colors = c("#6D9EC1", "white", "#E46726")
#)
plotly::ggplotly(ggcorrplot(corr,outline.color = "black",colors = c("salmon","white","skyblue2"),lab = T)+ggtitle("Correlation plot\n"))
```

Almost all continuous variables are uncorrelated thereby satisfying absence of multi-collinearity assumption of logistic regression.

```{r}
#Check impactables with simple logistic regression
model.glm <- glm(Exited~.,df,family = "binomial")
summary(model.glm)


```
In addition to demographics such as age, gender and location,  actionable variables such as Credit score, Customer balance and active membership are strong predictors. Given above p-values might change as per number of variables, we try running stepwise regression to better understand best combination of features that we can use.

```{r}
step.model <- model.glm %>% stepAIC(trace = FALSE)
cols <- names(coef(step.model))
summary(step.model)

```


```{r}
#Setting up train validation test 
# Split the data into training and test set
set.seed(123)

training.samples <- df$Exited %>%  createDataPartition(p = 0.9,list = F)
train.data  <- df[training.samples, ]
test.data <- df[-training.samples, ]

# Define training control
train.control <- trainControl(## 10-fold repeated CV
                           method = "repeatedcv",
                           number = 10,repeats = 10)
# Train the model
train.data <- train.data[, colnames(train.data) %in% c(cols,"Exited")]
test.data <- test.data[, colnames(test.data) %in% c(cols,"Exited")]

cv_logit <- train(data = train.data, Exited~.,
                 method = "glm", 
                 family = "binomial",
                 trControl = train.control)
summary(cv_logit)

PredTrain = predict(cv_logit, newdata=train.data[,!colnames(train.data) == "Exited"], type="prob")
table(train.data$Exited, PredTrain[,2] > 0.5)

```


```{r fig.width=6.5, fig.height=5.5}
precrec_obj <- evalmod(scores = PredTrain[,2], labels = train.data$Exited)
autoplot(precrec_obj)
#precrec_obj <- evalmod(scores = PredTrain[,2], labels = train.data$Exited,mode="basic")
#autoplot(precrec_obj)

```



```{r}
library(tree)

# train = sample(1:nrow(df),7000)
# churn.test=df[-train,]
# Exited.test=df$Exited[-train]
# df.test=df[-train,"Exited"]
# tree.churn = tree(Exited~.,df,subset=train)
# tree.pred = predict(tree.churn,churn.test,type="class")
# table(tree.pred,Exited.test)
# plot(tree.churn)
# text(tree.churn,pretty=0)
# library(randomForest)
# set.seed(100)
# rf.churn = randomForest(Exited~.,data=df,subset=train,importance=T,mtry=3,ntree=25)
# yhat.rf= predict(rf.churn,newdata=churn.test,type="response")
# mean((yhat.rf-df.test)^2)




library(tree)
library(randomForest)
set.seed(100)
train = sample(1:nrow(df),7000)
churn.train=df[train,]
churn.test=df[-train,]
#mtry = 3
churn.rf1 = randomForest(Exited~.,data=churn.train,ntree=100,proximity=T,importance=T)
table(predict(churn.rf1),churn.train$Exited)
churn.pred1 = predict(churn.rf1,newdata = churn.test)
OOSPred1 = table(churn.pred1,churn.test$Exited)

#importance(churn.rf)
varImpPlot(churn.rf1)
accuracy1=(sum(diag(OOSPred1)))/sum(OOSPred1)
accuracy1

plot(tree(churn.rf1))
text(tree(churn.rf1),pretty=0)

#mtry = 4
set.seed(100)

churn.rf2 = randomForest(Exited~.,data=churn.train,ntree=100,mtry = 4, proximity=T,importance=T)
churn.pred2 = predict(churn.rf2,newdata = churn.test)
OOSPred2 = table(churn.pred2,churn.test$Exited)
accuracy2=(sum(diag(OOSPred2)))/sum(OOSPred2)
accuracy2

#mtry = 2
set.seed(100)

churn.rf3 = randomForest(Exited~.,data=churn.train,ntree=100,mtry = 2, proximity=T,importance=T)
churn.pred3 = predict(churn.rf3,newdata = churn.test)
OOSPred3 = table(churn.pred3,churn.test$Exited)
accuracy3=(sum(diag(OOSPred3)))/sum(OOSPred3)
accuracy3

```

```{r}

# Gradient Boosting Machines (GBM)

df$Geography <- as.factor(df$Geography)
df$Gender <- as.factor(df$Gender)
df$HasCrCard <- as.factor(df$HasCrCard)
df$IsActiveMember <- as.factor(df$IsActiveMember)


set.seed(234)
split_data <- createDataPartition(df$Exited, p=0.7, list = FALSE)
train_data <- df[split_data,]
test_data <- df[-split_data,]

# GBM model without cross validation ; Important features: Age, Balance, NumOfProducts, EstimatedSalary

gbmmodel <- gbm(Exited ~.,distribution="bernoulli",data=train_data,n.trees=1000,interaction.depth = 3)
summary(gbmmodel)


print(gbmmodel)

# GBM model without cross validation ; Important features: Age, Balance, NumOfProducts, EstimatedSalary, CreditScore
# With shrinkage (learning rate) the computation time increases and so does the number of trees. 
# To counter the same we use a simple tree as a base model and interaction depth i.e. depth of trees as 3 to make the model less complex.

gbmmodel1 <- gbm(Exited ~.,
                 distribution="bernoulli",
                 data=train_data,
                 n.trees=1000,
                 interaction.depth = 3,
                 #shrinkage = 0.001,
                 cv.folds = 3)
                 
summary(gbmmodel1)
print(gbmmodel1)

rmse = sqrt(min(gbmmodel1$cv.error))
rmse

optm_cv <- gbm.perf(gbmmodel1,method="cv")
optm_oob <- gbm.perf(gbmmodel1,method="OOB")

print(optm_cv)
print(optm_oob)


predictions <- predict(object = gbmmodel1,
                       newdata = test_data,
                       n.trees = optm_cv,
                       type = "response")


# Selecting cutoff probability of churn as 0.3.

binarypredictions <- as.factor(ifelse(predictions>0.3,1,0))
test_data$Exited <- as.factor(test_data$Exited)
confusionMatrix(binarypredictions,test_data$Exited)

gbm_pred_test <- prediction(predictions,test_data$Exited)
gbm_roc_testing <- performance(gbm_pred_test,"tpr","fpr")
plot(gbm_roc_testing)


# AUC is 86.7% indicating a good separation between the two classes of churn

auc_temp <- performance(gbm_pred_test,"auc")
gbm_auc_testing <- as.numeric(auc_temp@y.values)
gbm_auc_testing

```









